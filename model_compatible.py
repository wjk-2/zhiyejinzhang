import pandas as pd
import numpy as np
import pickle
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, f1_score
from sklearn.preprocessing import StandardScaler
import os

class CompatibleOccupationalStressModel:
    """å…¼å®¹ç‰ˆæœ¬çš„èŒä¸šç´§å¼ æ¨¡å‹ï¼Œé¿å…ç‰ˆæœ¬å†²çª"""
    
    def __init__(self):
        self.model = None
        self.scaler = None
        self.feature_names = ['å¹´é¾„', 'å·¥é¾„', 'æœ¬å²—ä½å·¥é¾„', 'å‘¨å‡å·¥ä½œæ—¶é—´', 'æ—¥å‡åŠ ç­æ—¶é—´',
                            'ç”Ÿæ´»æ»¡æ„åº¦å¾—åˆ†', 'ç–²åŠ³ç¨‹åº¦åˆ†çº§', 'æ”¶å…¥æ°´å¹³', 'é¥®é…’é‡', 
                            'ä½å¼ºåº¦é”»ç‚¼', 'å¸çƒŸé‡', 'å©šå§»çŠ¶å†µ']
        self.model_path = 'models/compatible_stress_model.pkl'
        self.scaler_path = 'models/compatible_scaler.pkl'
        
    def load_data(self, excel_path='stress_data.xlsx'):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        # ç¡®ä¿modelsç›®å½•å­˜åœ¨
        os.makedirs('models', exist_ok=True)
        
        # è¯»å–æ•°æ®
        df = pd.read_excel(excel_path)
        
        # å¤„ç†ç¼ºå¤±å€¼
        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
        for col in numeric_cols:
            if df[col].isnull().sum() > 0:
                df[col] = df[col].fillna(df[col].median())
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = df[self.feature_names]
        y = df['æ˜¯å¦èŒä¸šç´§å¼ '].astype(int)
        
        # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆæ‰‹åŠ¨å®ç°é‡é‡‡æ ·ï¼‰
        class_0_count = (y == 0).sum()
        class_1_count = (y == 1).sum()
        
        if class_1_count < class_0_count:
            # å¯¹å°‘æ•°ç±»è¿›è¡Œä¸Šé‡‡æ ·
            minority_indices = y[y == 1].index
            oversampled_indices = np.random.choice(
                minority_indices, 
                size=class_0_count - class_1_count, 
                replace=True
            )
            X_oversampled = pd.concat([X, X.loc[oversampled_indices]])
            y_oversampled = pd.concat([y, y.loc[oversampled_indices]])
            X, y = X_oversampled, y_oversampled
        
        # æ•°æ®æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        with open(self.scaler_path, 'wb') as f:
            pickle.dump(self.scaler, f)
            
        return X_scaled, y
    
    def optimize_hyperparameters(self, X, y):
        """æ‰‹åŠ¨å®ç°è¶…å‚æ•°ä¼˜åŒ–"""
        best_score = 0
        best_params = {}
        
        # ç®€å•çš„å‚æ•°æœç´¢
        param_combinations = [
            {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2},
            {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 5},
            {'n_estimators': 300, 'max_depth': 20, 'min_samples_split': 10},
            {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2},
        ]
        
        for params in param_combinations:
            # 5æŠ˜äº¤å‰éªŒè¯
            scores = []
            for _ in range(5):
                X_train, X_val, y_train, y_val = train_test_split(
                    X, y, test_size=0.2, random_state=np.random.randint(1000)
                )
                
                model = RandomForestClassifier(
                    n_estimators=params['n_estimators'],
                    max_depth=params['max_depth'],
                    min_samples_split=params['min_samples_split'],
                    random_state=42,
                    class_weight='balanced'
                )
                model.fit(X_train, y_train)
                y_pred = model.predict(X_val)
                score = f1_score(y_val, y_pred, average='weighted')
                scores.append(score)
            
            avg_score = np.mean(scores)
            if avg_score > best_score:
                best_score = avg_score
                best_params = params
        
        print(f"æœ€ä½³F1åˆ†æ•°: {best_score:.3f}")
        print(f"æœ€ä½³å‚æ•°: {best_params}")
        
        # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        final_model = RandomForestClassifier(
            n_estimators=best_params['n_estimators'],
            max_depth=best_params['max_depth'],
            min_samples_split=best_params['min_samples_split'],
            random_state=42,
            class_weight='balanced'
        )
        
        return final_model
    
    def train_model(self, excel_path='stress_data.xlsx'):
        """è®­ç»ƒå…¼å®¹ç‰ˆæœ¬çš„æ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒå…¼å®¹æ¨¡å‹...")
        
        # ç¡®ä¿modelsç›®å½•å­˜åœ¨
        os.makedirs('models', exist_ok=True)
        
        # åŠ è½½æ•°æ®
        X, y = self.load_data(excel_path)
        
        # åˆ’åˆ†æ•°æ®é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # ä¼˜åŒ–è¶…å‚æ•°
        self.model = self.optimize_hyperparameters(X_train, y_train)
        
        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        self.model.fit(X_train, y_train)
        
        # è¯„ä¼°æ¨¡å‹
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"æµ‹è¯•é›†F1åˆ†æ•°: {f1:.3f}")
        print(classification_report(y_test, y_pred))
        
        # ä¿å­˜æ¨¡å‹
        with open(self.model_path, 'wb') as f:
            pickle.dump(self.model, f)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {self.model_path}")
        return accuracy
    
    def predict_stress(self, input_data):
        """é¢„æµ‹èŒä¸šç´§å¼ é£é™©"""
        if self.model is None:
            self.load_model()
        
        if isinstance(input_data, dict):
            input_df = pd.DataFrame([input_data])
            input_df = input_df[self.feature_names]
            
            # ä½¿ç”¨ä¿å­˜çš„æ ‡å‡†åŒ–å™¨
            if os.path.exists(self.scaler_path):
                with open(self.scaler_path, 'rb') as f:
                    scaler = pickle.load(f)
                input_scaled = scaler.transform(input_df)
            else:
                input_scaled = input_df.values
        else:
            input_scaled = input_data
        
        probability = self.model.predict_proba(input_scaled)[0]
        prediction = self.model.predict(input_scaled)[0]
        
        risk_prob = probability[1]
        if risk_prob < 0.3:
            risk_level = "ä½é£é™©"
        elif risk_prob < 0.7:
            risk_level = "ä¸­é£é™©"
        else:
            risk_level = "é«˜é£é™©"
            
        return {
            'prediction': int(prediction),
            'probability': risk_prob,
            'risk_level': risk_level,
            'confidence': max(probability)
        }
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if os.path.exists(self.model_path):
            with open(self.model_path, 'rb') as f:
                self.model = pickle.load(f)
        else:
            print("âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
            self.train_model()
    
    def get_feature_importance(self):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if self.model is None:
            self.load_model()
        
        importance = self.model.feature_importances_
        return sorted(zip(self.feature_names, importance), 
                     key=lambda x: x[1], reverse=True)

def process_user_input(user_input):
    """å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œè½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„æ ¼å¼"""
    processed = {}
    
    # å¹´é¾„ã€å·¥é¾„ç­‰ç›´æ¥ä½¿ç”¨
    processed['å¹´é¾„'] = user_input.get('age', 0)
    processed['å·¥é¾„'] = user_input.get('work_years', 0)
    processed['æœ¬å²—ä½å·¥é¾„'] = user_input.get('position_years', 0)
    processed['å‘¨å‡å·¥ä½œæ—¶é—´'] = user_input.get('weekly_hours', 40)
    processed['æ—¥å‡åŠ ç­æ—¶é—´'] = user_input.get('daily_overtime', 0)
    processed['é¥®é…’é‡'] = user_input.get('alcohol', 0)
    processed['ä½å¼ºåº¦é”»ç‚¼'] = user_input.get('low_exercise', 0)
    processed['ç”Ÿæ´»æ»¡æ„åº¦å¾—åˆ†'] = user_input.get('life_satisfaction', 5)
    processed['ç–²åŠ³ç¨‹åº¦åˆ†çº§'] = user_input.get('fatigue_level', 0)
    processed['å¸çƒŸé‡'] = user_input.get('smoking', 0)
    
    # åˆ†ç±»å˜é‡è½¬æ¢
    income_mapping = {'ä½': -1, 'ä¸­': 0, 'é«˜': 1}
    processed['æ”¶å…¥æ°´å¹³'] = income_mapping.get(user_input.get('income', 'ä¸­'), 0)
    
    education_mapping = {'é«˜ä¸­åŠä»¥ä¸‹': 0, 'å¤§ä¸“': 1, 'æœ¬ç§‘': 2, 'ç¡•å£«åŠä»¥ä¸Š': 3}
    processed['æ•™è‚²ç¨‹åº¦'] = education_mapping.get(user_input.get('education', 'æœ¬ç§‘'), 2)
    
    marriage_mapping = {'æœªå©š': 1, 'å·²å©šåŒå±…': 2, 'å·²å©šåˆ†å±…': 3, 'ç¦»å©š': 4, 'ä¸§å¶': 5}
    processed['å©šå§»çŠ¶å†µ'] = marriage_mapping.get(user_input.get('marital_status', 'æœªå©š'), 1)
    
    return processed

# è®­ç»ƒå…¼å®¹æ¨¡å‹
if __name__ == "__main__":
    print("ğŸ¯ èŒä¸šç´§å¼ æ¨¡å‹å…¼å®¹ç‰ˆæœ¬")
    print("=" * 50)
    
    model = CompatibleOccupationalStressModel()
    accuracy = model.train_model()
    
    print("=" * 50)
    print(f"âœ… å…¼å®¹æ¨¡å‹è®­ç»ƒå®Œæˆï¼å‡†ç¡®ç‡: {accuracy:.3f}")